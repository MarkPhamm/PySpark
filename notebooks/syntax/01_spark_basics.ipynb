{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark 01 — Spark Basics\n",
    "\n",
    "This notebook introduces the basics of working with Apache Spark using PySpark. Topics covered:\n",
    "\n",
    "1. Creating a SparkSession\n",
    "2. Constructing a DataFrame from Python data\n",
    "3. Exploring DataFrame structure\n",
    "4. Viewing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a Spark Session\n",
    "The entry point to Spark is the SparkSession. It allows access to all Spark features from PySpark.\n",
    "\n",
    "In PySpark, the **`SparkSession`** is part of the **Structured API** (which includes DataFrames and SQL-like operations). That API lives in the `pyspark.sql` module — even though you're not necessarily writing SQL.\n",
    "\n",
    "**Modules**\n",
    "```\n",
    "pyspark/\n",
    "├── sql/\n",
    "│   ├── SparkSession.py     <-- defines SparkSession class\n",
    "│   ├── DataFrame.py\n",
    "│   └── functions.py        <-- for withColumn, col, etc.\n",
    "├── rdd/\n",
    "│   └── RDD.py              <-- lower-level API\n",
    "```\n",
    "\n",
    "* `pyspark.sql`: includes all higher-level DataFrame and SQL APIs\n",
    "* `pyspark.rdd`: older, low-level RDD API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What’s Inside a SparkSession?\n",
    "Internally, SparkSession wraps:\n",
    "* A SparkContext (the original core interface to the cluster)\n",
    "* A SQLContext (for DataFrames and SQL)\n",
    "* A HiveContext (if working with Hive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Basics\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://idx-pyspark-1746386305122:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Basics>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check the Cluster Info (Number of Nodes/Executors/Cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: 1\n",
      "Executor Hosts: ['idx-pyspark-1746386305122:38439']\n",
      "Default Parallelism (cores): 2\n"
     ]
    }
   ],
   "source": [
    "# Get Java Set of executor memory keys\n",
    "java_set = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet()\n",
    "\n",
    "# Use Java iterator to safely loop through elements\n",
    "hosts_iter = java_set.iterator()\n",
    "hosts = []\n",
    "\n",
    "while hosts_iter.hasNext():\n",
    "    hosts.append(str(hosts_iter.next()))\n",
    "\n",
    "cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Display results\n",
    "print(\"Executors:\", len(hosts))\n",
    "print(\"Executor Hosts:\", hosts)\n",
    "print(\"Default Parallelism (cores):\", cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing CPU Parallelism in Spark**\n",
    "\n",
    "Pros\n",
    "- **Faster execution** — more tasks run simultaneously\n",
    "- **Better hardware use** — maximizes CPU utilization\n",
    "- **Handles larger data** — improved performance on big workloads\n",
    "- **Closer to real clusters** — mimics distributed behavior in dev\n",
    "\n",
    "Cons\n",
    "- **Higher memory use** — more threads = more memory pressure\n",
    "- **Wasteful for small jobs** — extra overhead with little gain\n",
    "- **Diminishing returns** — beyond a point, no added benefit\n",
    "- **Can affect other apps** — may slow down your system\n",
    "\n",
    "Use `local[*]` to automatically match the number of available cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all available cores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParallelTest\") \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: 1\n",
      "Executor Hosts: ['idx-pyspark-1746386305122:37453']\n",
      "Default Parallelism (cores): 16\n"
     ]
    }
   ],
   "source": [
    "# Get Java Set of executor memory keys\n",
    "java_set = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet()\n",
    "\n",
    "# Use Java iterator to safely loop through elements\n",
    "hosts_iter = java_set.iterator()\n",
    "hosts = []\n",
    "\n",
    "while hosts_iter.hasNext():\n",
    "    hosts.append(str(hosts_iter.next()))\n",
    "\n",
    "cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Display results\n",
    "print(\"Executors:\", len(hosts))\n",
    "print(\"Executor Hosts:\", hosts)\n",
    "print(\"Default Parallelism (cores):\", cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Firebase Studio is single-node (like your laptop), it won't spin up a real Spark cluster — it just simulates one for development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Constructing a DataFrame from Python data\n",
    "You can create a DataFrame directly from a list of tuples and specify column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 32|\n",
      "|Cathy| 19|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sample Python data (list of tuples)\n",
    "data = [\n",
    "    (\"Alice\", 25),\n",
    "    (\"Bob\", 32),\n",
    "    (\"Cathy\", 19)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for small tasks, Spark is starting a whole execution engine under the hood, which includes:\n",
    "* Starting the JVM (Java Virtual Machine)\n",
    "* Initializing SparkContext\n",
    "* Creating a thread pool\n",
    "* Allocating memory for executors\n",
    "* Logging setup and environment scanning\n",
    "* This startup cost can easily take 10–20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Dan| 40|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(\"Dan\", 40)], [\"name\", \"age\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring DataFrame structure\n",
    "After creating a DataFrame, you can inspect its schema, column names, data types, and sample rows using these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema (column names and data types)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get (column name, data type) pairs\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Viewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 32|\n",
      "|Cathy| 19|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display first few rows in a tabular format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=25), Row(name='Bob', age=32), Row(name='Cathy', age=19)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows as Row objects\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
