{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SparkSession: The Entry Point to Spark\n",
    "\n",
    "SparkSession serves as the unified entry point to all Spark functionality in Spark 2.0 and later. It encapsulates SparkContext, SQLContext, HiveContext, and StreamingContext into a single interface, making Spark programming more streamlined and intuitive.\n",
    "\n",
    "The entry point to Spark is the SparkSession. It allows access to all Spark features from PySpark. In PySpark, the **`SparkSession`** is part of the **Structured API** (which includes DataFrames and SQL-like operations). That API lives in the `pyspark.sql` module — even though you're not necessarily writing SQL.\n",
    "\n",
    "```\n",
    "pyspark/\n",
    "├── sql/\n",
    "│   ├── SparkSession.py     <-- defines SparkSession class\n",
    "│   ├── DataFrame.py        <-- DataFrame operations\n",
    "│   └── functions.py        <-- for withColumn, col, etc.\n",
    "├── rdd/\n",
    "│   └── RDD.py              <-- lower-level API\n",
    "```\n",
    "\n",
    "The PySpark API is organized into key modules:\n",
    "* `pyspark.sql`: includes all higher-level DataFrame and SQL APIs\n",
    "* `pyspark.rdd`: older, low-level RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/07 18:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"SafeBind\")\n",
    "      .master(\"local[*]\")\n",
    "      # Bind and advertise on loop‑back only\n",
    "      .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "      .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "      .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SparkSession Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Component Hierarchy\n",
    "\n",
    "SparkSession sits at the top of the Spark architectural hierarchy:\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "    ├── SparkContext\n",
    "    │   └── Scheduler\n",
    "    │       ├── DAGScheduler\n",
    "    │       └── TaskScheduler\n",
    "    ├── SQLContext\n",
    "    │   └── Catalyst Optimizer\n",
    "    └── StreamingContext (optional)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Core Components\n",
    "\n",
    "### 2.2.1 What's Inside a SparkSession?\n",
    "\n",
    "Internally, SparkSession wraps:\n",
    "* A SparkContext (the original core interface to the cluster)\n",
    "* A SQLContext (for DataFrames and SQL)\n",
    "* A HiveContext (if working with Hive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master: local[*]\n",
      "App Name: SafeBind\n",
      "Spark Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Access underlying SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# View SparkContext details\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"App Name: {sc.appName}\")\n",
    "print(f\"Spark Version: {sc.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SafeBind</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=SafeBind>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 SQLContext\n",
    "\n",
    "The SQLContext handles all SQL and DataFrame operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|John| 30|\n",
      "| Bob| 45|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"John\", 30), (\"Alice\", 25), (\"Bob\", 45)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Register as temporary view to use with SQL\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Now we can use SQL\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 25\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Catalog\n",
    "The Catalog interface provides access to tables, databases, and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      "  - my_table (isTemporary: True)\n",
      "  - people (isTemporary: True)\n",
      "\n",
      "Databases:\n",
      "  - default\n",
      "\n",
      "Functions (first 5):\n",
      "  - ! (isSystem: True)\n",
      "  - != (isSystem: True)\n",
      "  - % (isSystem: True)\n",
      "  - & (isSystem: True)\n",
      "  - * (isSystem: True)\n"
     ]
    }
   ],
   "source": [
    "# Register as a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Now we can list tables (will show \"people\")\n",
    "tables = spark.catalog.listTables()\n",
    "print(\"Tables:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table.name} (isTemporary: {table.isTemporary})\")\n",
    "\n",
    "# List all databases (at minimum will have \"default\")\n",
    "databases = spark.catalog.listDatabases()\n",
    "print(\"\\nDatabases:\")\n",
    "for db in databases:\n",
    "    print(f\"  - {db.name}\")\n",
    "\n",
    "# List all functions (system and user-defined)\n",
    "functions = spark.catalog.listFunctions()\n",
    "print(\"\\nFunctions (first 5):\")\n",
    "for i, function in enumerate(functions[:5]):\n",
    "    print(f\"  - {function.name} (isSystem: {function.isTemporary})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SparkSession Creation & Configuration\n",
    "SparkSession uses the builder pattern for flexible configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Builder Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping Older Session\n",
    "spark.stop()\n",
    "\n",
    "# create new Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 CPU Parallelism Configuration\n",
    "\n",
    "When setting the master URL with `local[n]`, you control the CPU parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:38:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/05/07 18:38:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Use 4 threads\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "\n",
    "# Use all available cores\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: 1\n",
      "Executor Hosts: ['127.0.0.1:39725']\n",
      "Default Parallelism (cores): 2\n"
     ]
    }
   ],
   "source": [
    "# Get Java Set of executor memory keys\n",
    "java_set = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet()\n",
    "\n",
    "# Use Java iterator to safely loop through elements\n",
    "hosts_iter = java_set.iterator()\n",
    "hosts = []\n",
    "\n",
    "while hosts_iter.hasNext():\n",
    "    hosts.append(str(hosts_iter.next()))\n",
    "\n",
    "cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Display results\n",
    "print(\"Executors:\", len(hosts))\n",
    "print(\"Executor Hosts:\", hosts)\n",
    "print(\"Default Parallelism (cores):\", cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "# Use all available cores, run the job with N worker threads in the JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParallelTest\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark doesn’t verify how many CPU cores you really have. If your machine has fewer than 256 cores, the extra threads just time‑share the available CPUs. If you have, say, 8 cores, the operating system schedules those 256 Spark threads onto the 8 cores—so they run in small slices. Performance usually does not improve beyond the real core count; in fact, too many threads can add overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: 1\n",
      "Executor Hosts: ['127.0.0.1:38601']\n",
      "Default Parallelism (cores): 2\n"
     ]
    }
   ],
   "source": [
    "# Get Java Set of executor memory keys\n",
    "java_set = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet()\n",
    "\n",
    "# Use Java iterator to safely loop through elements\n",
    "hosts_iter = java_set.iterator()\n",
    "hosts = []\n",
    "\n",
    "while hosts_iter.hasNext():\n",
    "    hosts.append(str(hosts_iter.next()))\n",
    "\n",
    "cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Display results\n",
    "print(\"Executors:\", len(hosts))\n",
    "print(\"Executor Hosts:\", hosts)\n",
    "print(\"Default Parallelism (cores):\", cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing CPU Parallelism in Spark**\n",
    "\n",
    "Pros:\n",
    "- **Faster execution** — more tasks run simultaneously\n",
    "- **Better hardware use** — maximizes CPU utilization\n",
    "- **Handles larger data** — improved performance on big workloads\n",
    "- **Closer to real clusters** — mimics distributed behavior in dev\n",
    "\n",
    "Cons:\n",
    "- **Higher memory use** — more threads = more memory pressure\n",
    "- **Wasteful for small jobs** — extra overhead with little gain\n",
    "- **Diminishing returns** — beyond a point, no added benefit\n",
    "- **Can affect other apps** — may slow down your system\n",
    "\n",
    "Use `local[*]` to automatically match the number of available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Configuration Sources\n",
    "\n",
    "SparkSession configurations can come from multiple sources (in order of precedence):\n",
    "1. Programmatic configuration in code\n",
    "2. Command line options\n",
    "3. Properties file (spark-defaults.conf)\n",
    "4. Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hive Integration\n",
    "\n",
    "Enable Hive support to use Hive features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive Integration Example\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SparkSession Deep Internals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Session State Management\n",
    "\n",
    "SparkSession maintains session state including:\n",
    "- Temporary views\n",
    "- UDFs registered in the session\n",
    "- In-memory catalog state\n",
    "- Session-level configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session-specific configurations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 100)\n",
    "current_value = spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Lazy Evaluation\n",
    "\n",
    "SparkSession operations follow Spark's lazy evaluation pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created - no computation yet\n",
      "Filter applied - still no computation\n",
      "Selection applied - still no computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action triggered - Count result: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second action triggered - Names collected: [Row(name='Bob'), Row(name='Charlie'), Row(name='Edward')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame directly without needing a file\n",
    "data = [(\"Alice\", 25), (\"Bob\", 35), (\"Charlie\", 45), (\"Diana\", 28), (\"Edward\", 50)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "# This creates a logical plan, but no execution yet\n",
    "df = spark.createDataFrame(data, columns)\n",
    "print(\"DataFrame created - no computation yet\")\n",
    "\n",
    "# This adds to the logical plan, still no execution\n",
    "df_filtered = df.filter(df.age > 30)\n",
    "print(\"Filter applied - still no computation\")\n",
    "\n",
    "# Let's add another transformation\n",
    "df_names = df_filtered.select(\"name\")\n",
    "print(\"Selection applied - still no computation\")\n",
    "\n",
    "# Action triggers execution of the entire plan\n",
    "result_count = df_filtered.count()\n",
    "print(f\"Action triggered - Count result: {result_count}\")\n",
    "\n",
    "# Another action will execute its plan separately\n",
    "names_list = df_names.collect()\n",
    "print(f\"Second action triggered - Names collected: {names_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates how Spark builds a logical execution plan through transformations (creating DataFrame, filter, select) but doesn't execute anything until an action (count, collect) is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Resource Management\n",
    "SparkSession coordinates with the cluster manager (YARN, Kubernetes, etc.) through SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism: 2\n",
      "Default Minimum Partitions for RDDs: 2\n",
      "Default Parallelism from conf: not set\n",
      "Executor Memory: 1g\n",
      "Executor Cores: not set\n"
     ]
    }
   ],
   "source": [
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# View current resource allocation\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Default Minimum Partitions for RDDs: {sc.defaultMinPartitions}\")\n",
    "\n",
    "# Alternative way to get default parallelism\n",
    "parallelism = spark.conf.get(\"spark.default.parallelism\", \"not set\")\n",
    "print(f\"Default Parallelism from conf: {parallelism}\")\n",
    "\n",
    "# Other resource configurations\n",
    "executor_memory = spark.conf.get(\"spark.executor.memory\", \"1g\")\n",
    "executor_cores = spark.conf.get(\"spark.executor.cores\", \"not set\")\n",
    "\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "print(f\"Executor Cores: {executor_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `defaultMinPartitions` isn't actually an attribute of SparkContext - it's used here to demonstrate a concept. In practice, you would check specific configurations that control minimum partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SparkSession Tuning & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Memory Configuration\n",
    "The code explicitly tunes Spark’s JVM heap:\n",
    "\n",
    "* `spark.driver.memory \"4g\"` → allocates 4 GB to the driver (the “head‑chef” process that plans jobs).\n",
    "* `spark.executor.memory \"8g\"` → gives each executor (worker) 8 GB for task execution.\n",
    "* `spark.memory.fraction 0.8` → lets tasks use up to 80 % of that heap for execution and caching.\n",
    "* `spark.memory.storageFraction 0.5` → reserves half of that 80 % for cached data (the rest is for shuffle and computation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:38:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Memory Configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Execution Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`spark.sql.shuffle.partitions = 200`**\n",
    "  Sets the default number of output partitions after wide operations (e.g., `groupBy`, `join`). More partitions mean smaller individual tasks but more scheduling overhead; tune this to match cluster size (e.g., cores × 2–3).\n",
    "\n",
    "* **`spark.sql.autoBroadcastJoinThreshold = \"10m\"`**\n",
    "  Any table ≤ 10 MB is automatically broadcast to all executors, turning large‑small joins into fast map‑side joins and avoiding costly shuffles.\n",
    "\n",
    "* **`spark.sql.adaptive.enabled = true`**\n",
    "  Activates Adaptive Query Execution (AQE): Spark can change join strategies, shuffle partition counts, and skew handling at run‑time based on actual data statistics, often yielding better performance without manual tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL operations tuning\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10m\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 I/O Optimization\n",
    "* **`spark.sql.files.maxPartitionBytes = \"128m\"`**\n",
    "  When Spark scans files (CSV/Parquet/JSON), it splits them into input partitions no larger than 128 MB. This balances work per task: big enough to keep each task busy, but small enough to create parallelism and avoid memory blow‑ups.\n",
    "\n",
    "* **`spark.sql.files.openCostInBytes = \"4m\"`**\n",
    "  A cost model hint: Spark treats opening a new file as equivalent to reading 4 MB of data. Raising or lowering this value influences how Spark coalesces tiny files into larger splits, helping mitigate the “small‑files problem.”\n",
    "\n",
    "* **`spark.sql.parquet.compression.codec = \"snappy\"`**\n",
    "  Uses Snappy compression for Parquet output. Snappy offers a good trade‑off: fast compression/decompression with moderate space savings, resulting in quicker I/O compared to heavier codecs like Gzip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O tuning\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", \"4m\")\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Dynamic Allocation\n",
    "* **`spark.dynamicAllocation.enabled = true`**\n",
    "  Turns on **dynamic executor allocation**: Spark automatically adds or removes executors at run‑time based on the workload’s current demand.\n",
    "\n",
    "* **`spark.dynamicAllocation.minExecutors = 2`**\n",
    "  Spark will never go below 2 executors, ensuring a baseline level of parallelism even during idle periods.\n",
    "\n",
    "* **`spark.dynamicAllocation.maxExecutors = 20`**\n",
    "  Caps the pool at 20 executors, preventing runaway cluster usage and keeping costs predictable.\n",
    "\n",
    "* **`spark.shuffle.service.enabled = true`**\n",
    "  Enables the external shuffle service; required for dynamic allocation so that shuffle files remain accessible when executors are removed and later tasks still need those files.\n",
    "\n",
    "Together, these settings let Spark flex between 2 and 20 executors, scaling up during peaks and shrinking when the job is less busy—while keeping shuffle data safe during the ups‑and‑downs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 2) \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 20) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Serialization Options\n",
    "* **`spark.serializer = \"org.apache.spark.serializer.KryoSerializer\"`**\n",
    "  Switches Spark from the default Java serializer to **Kryo**, which is far more compact and faster to (de)serialize. This reduces network traffic and memory footprint, improving shuffle and cache performance.\n",
    "\n",
    "* **`spark.kryo.registrationRequired = false`**\n",
    "  Lets Spark serialize any class on‑the‑fly without pre‑registering it. Keeping this `false` is convenient (no manual class list), but registering frequently‑used custom classes can squeeze out even more speed and space efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 18:38:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Use Kryo serialization for better performance\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrationRequired\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
